---
title: "Replication Crisis Notebook"
output: html_notebook
---

Loading libraries that will be used during this process. 
```{r}
library(dplyr)
library(tidyr)
library(ggplot2)
library(readr)
library(stringr)
library(skimr)
library(splitstackshape)
library(janitor)
```

Loading the csv file

```{r}
#loading the csv files and then joining them in to one data base
  ##converting blanks in to NA

#2010 data bases
data.10.o <- read.csv ("2010_no_articles.csv", na.strings = c("", "NA"))
data.10.a <- read.csv ("2010_yes_articles.csv", na.strings = c("", "NA"))

#2011 data bases
data.11.o <- read.csv ("2011_no_articles.csv", na.strings = c("", "NA"))
data.11.a <- read.csv ("2011_yes_articles.csv", na.strings = c("", "NA"))

#2012 databases
data.12.o <- read.csv ("2012_no_articles.csv", na.strings = c("", "NA"))
data.12.a <- read.csv ("2012_yes_articles.csv", na.strings = c("", "NA"))

#2013 databases
data.13.o <- read.csv ("2013_no_articles.csv", na.strings = c("", "NA"))
data.13.a <- read.csv ("2013_articles.csv", na.strings = c("", "NA"))

#2014 databases
data.14.a <- read.csv ("2014_articles.csv", na.strings = c("", "NA"))
data.14.o <- read.csv ("2014_other.csv", na.strings = c("", "NA"))

#2015 databases
data.15.a <- read.csv ("2015_articles.csv", na.strings = c("", "NA"))
data.15.o <- read.csv ("2015_other.csv", na.strings = c("", "NA"))

#2016 database
  ## everything fit in one for 2016
data.16.all <- read.csv ("2016_all.csv", na.strings = c("", "NA"))

#2017 database
  ## everything fir in one for 2017
data.17.all <- read.csv ("2017_all.csv", na.strings = c("", "NA"))

#2018 database
data.18.a <- read.csv ("2018_articles.csv", na.strings = c("", "NA"))
data.18.o <- read.csv ("2018_other.csv", na.strings = c("", "NA"))

#2019 database
data.19.a <- read.csv ("2019_articles.csv", na.strings = c("", "NA"))
data.19.n <- read.csv ("2019_notes.csv", na.strings = c("", "NA"))
data.19.o <- read.csv ("2019_other.csv", na.strings = c("", "NA"))

#2020 database
data.20.n <- read.csv ("2020_notes.csv", na.strings = c("", "NA"))
data.20.o <- read.csv ("2020_other_articles.csv", na.strings = c("", "NA"))
```

```{r}
#putting the databases together into one
data.original <- rbind(data.10.a, data.10.o,
          data.11.a, data.11.o, 
          data.12.a, data.12.o, 
          data.13.a, data.13.a,
          data.14.a, data.14.o,
          data.15.a, data.15.o,
          data.16.all,
          data.17.all,
          data.18.o, data.18.a,
          data.19.a, data.19.n, data.19.o,
          data.20.n, data.20.o)
```


```{r include=FALSE}
#cleaning the csv file
  ## discarding some of the columns that won't be used
  ## renaming the column titles so they're not capitalized
  ## converting lack of information notes in to NA strings
  ## adding an id to each column

data.original <- data.original %>%
 select(-Art..No., -Molecular.Sequence.Numbers, -Chemicals.CAS,-Tradenames, -Manufacturers, -Funding.Text.1, -Funding.Text.2, -Funding.Text.3, -Funding.Text.4, -Funding.Text.5, -Funding.Text.6, -Funding.Text.7, -Funding.Text.8, -Funding.Text.9, -Funding.Text.10, -Correspondence.Address,  -Abbreviated.Source.Title)
```

```{r}
#renaming the columns for easier handling
names(data.original) <- c("authors", "author.id", "title", "year", "source.title", "volume", "issue", "page.start", "page.end", "page.count", "cited.by", "doi", "link", "affiliations", "author.affiliations", "abstract", "author.keywords", "index.keywords", "funding.details", "references", "editors", "sponsors", "publisher", "conference.name", "conference.date", "conference.location", "conference.code", "issn", "isbn", "coden", "pubmed.id", "original.language", "document.type", "publication.stage", "open.access", "source", "eid")
```

```{r}
#extracting the unique instances
data.original <- unique(data.original)
```

```{r}
#checking how many NAs exist within each column
colSums(is.na(data.original))

##UPDATE HERE WHAT THE NUMBERS ARE BEFORE I RUN THEM
  ##RETRACT COLUMNS THAT ARE TOTALLY EMPTY

#all columns (except isbn) are totally empty columns
  #isbn has one 1 row with data
data.original <- data.original %>%
 select(-editors, -sponsors, -conference.name, -conference.date, -conference.location, -conference.code)
```

```{r}
#adding a unique id code for all of the columns
data <- data.original %>%
  mutate(id = row_number())
```

```{}
#checking which columns use "[No x available]" format
  ##to be able to copy that format and replace with NA instead

data %>% filter(grepl("available]", authors))
#[No author name available]

data %>% filter(grepl("available]", author.id))
#[No author id available]

#none of these have inserted categories
data %>% filter(grepl("available]", title))
data %>% filter(grepl("available]", year))
data %>% filter(grepl("available]", source.title))
data %>% filter(grepl("available]", volume))
data %>% filter(grepl("available]", issue))
data %>% filter(grepl("available]", page.start))
data %>% filter(grepl("available]", page.end))
data %>% filter(grepl("available]", page.count))
data %>% filter(grepl("available]", cited.by))
data %>% filter(grepl("available]", doi))
data %>% filter(grepl("available]", link))
data %>% filter(grepl("available]", affiliations))
data %>% filter(grepl("available]", author.affiliations))
data %>% filter(grepl("available]", abstract))
#[No abstract available]

data %>% filter(grepl("available]", author.keywords))
data %>% filter(grepl("available]", index.keywords))
data %>% filter(grepl("available]", funding.details))
data %>% filter(grepl("available]", references))
data %>% filter(grepl("available]", publisher))
data %>% filter(grepl("available]", issn))
data %>% filter(grepl("available]", isbn))
data %>% filter(grepl("available]", coden))
data %>% filter(grepl("available]", pubmed.id))
data %>% filter(grepl("available]", original.language))
data %>% filter(grepl("available]", document.type))
data %>% filter(grepl("available]", publication.stage))
data %>% filter(grepl("available]", open.access))
data %>% filter(grepl("available]", source))
data %>% filter(grepl("available]", eid))
```


```{r}
#replacing text about missing data for NA instead in the columns that use text to indicate missing information
data <- data %>%
  mutate(authors = na_if(authors, "[No author name available]")) %>%
  mutate(author.id = na_if(author.id, "[No author id available]")) %>%
  mutate(abstract = na_if(abstract, "[No abstract available]"))
```


```{r}
#calculating the number of articles by volume/year
number.volumes <- data %>% count(volume)
#renaming
names(number.volumes) <- c("volume", "n.items.by.year")

#calculating the number of articles by issue
number.issues <- data %>% count(issue)
#renaming
names(number.issues) <- c("volume", "n.items.by.issue")

#calculating the number or issues by year
number.issues.by.year <- data %>%
  group_by(year) %>%
  count(issue) %>%
  select(year, issue) %>%
  count(year)

#renaming the column to be representative of content, number of issues
names(number.issues.by.year) <- c("year", "n.of.issues")
```


```{}
# identifying tiles, abstracts, author keywords and index keywords with my own keywords
  ## keywords: Replication crisis, replication crisis, Replicability crisis, replicability crisis, Reproducibility crisis, reproducibility crisis

data.title <- data %>%
  filter(grepl("Replication crisis|replication crisis|Replicability crisis|replicability crisis|Reproducibility crisis|reproducibility crisis", title))

data.abstract <- data %>%
  filter(grepl("Replication crisis|replication crisis|Replicability crisis|replicability crisis|Reproducibility crisis|reproducibility crisis", abstract))

data.author.keywords <- data %>%
  filter(grepl("Replication crisis|replication crisis|Replicability crisis|replicability crisis|Reproducibility crisis|reproducibility crisis", author.keywords))

data.index.keywords <- data %>%
  filter(grepl("Replication crisis|replication crisis|Replicability crisis|replicability crisis|Reproducibility crisis|reproducibility crisis", index.keywords))

#putting all of the categories back together 
data.replication <- do.call("rbind", list(data.title, data.abstract, data.author.keywords, data.index.keywords))

#deleting any articles that might have replicated
data.replication <- data.replication %>%
  distinct(id, .keep_all = TRUE)
```

```{r}
# identifying tiles, abstracts, author keywords and index keywords with my own keywords
  ## keywords: Replication crisis, replication crisis, Replicability crisis, replicability crisis, Reproducibility crisis, reproducibility crisis

data.title <- data %>%
  filter(grepl("Neural network|Neural networks|artificial neural network|artificial neural networks", title, ignore.case = TRUE))

data.abstract <- data %>%
  filter(grepl("Neural network|Neural networks|artificial neural network|artificial neural networks", abstract, ignore.case = TRUE))

data.author.keywords <- data %>%
  filter(grepl("Neural network|Neural networks|artificial neural network|artificial neural networks", author.keywords, ignore.case = TRUE))

data.index.keywords <- data %>%
  filter(grepl("Neural network|Neural networks|artificial neural network|artificial neural networks", index.keywords, ignore.case = TRUE))

#BINDING WITH THE EXTRA COLUMNS
#putting all of the categories back together 
data.networks <- do.call("rbind", list(data.title, data.abstract, data.author.keywords, data.index.keywords))

#deleting any articles that might have replicated
data.networks <- data.networks %>%
  distinct(id, .keep_all = TRUE)
```


```{r}
#splitting author.id, 
  ##in to separate columns using the splitstackshape library, which doesn't require you to know the total a column needs to be split in

data.networks <- cSplit(data.networks, "author.id", sep=";")

data.networks <-  cSplit(data.networks, "authors", sep=",")
  
data.networks <-  cSplit(data.networks, "affiliations", sep=";")

data.networks <- cSplit(data.networks, "index.keywords", sep=";")
  
data.networks <- cSplit(data.networks, "references", sep=";")

data.networks <- cSplit(data.networks, "author.keywords", sep=";")
```

Descriptive Data Visualization

```{r}
#creating a color theme for the visualizations
theme_networks <- theme(panel.grid = element_blank(),
                        panel.background = element_blank(),
                        panel.border = element_rect(color="black", fill = NA))

```

```{r}
#visualizing distribution of the citation counts for ALL articles in scien
#descriptive visualization
  ## one: histogram of the citation, bins with 30 counts
citation.hist.all <- ggplot(data, aes(x=cited.by)) +
  geom_histogram(fill = "#9999CC", binwidth= 30, center = 0.1) +
  xlab("Citation counts for all articles in Science") + 
  ylab("Number of articles")

citation.hist.all + theme_networks
```

```{r}
#descriptive visualization
  ## one: histogram of the citation, bins with 30 counts
citation.hist <- ggplot(data.networks, aes(x=cited.by)) +
  geom_histogram(fill = "#9999CC", binwidth= 30, center = 0.1) +
  scale_x_continuous(breaks = c(0, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 1100, 1200, 1300, 1400, 1500, 1600, 1700, 1800, 1900)) +
  xlab("Citation counts") + 
  ylab("Number of articles")

citation.hist + theme_networks
```


```{r}
#two: visualization of the citation counts excluding outliers (<1000)
  ## binwidth 10
data.networks.citation.most <- data.networks %>%
  filter(cited.by < 1000)

citation.hist.most <- ggplot(data.networks.citation.most, aes(x=cited.by), binwidth(10)) +
  geom_histogram(fill = "#9999CC") +
    scale_x_continuous(breaks = c(10, 50, 100, 150, 200, 250, 300, 350, 400, 
                                  450, 500, 550, 600, 650, 700, 750, 800)) +
  xlab("Citation counts, without outliers") + 
  ylab("Number of articles")

citation.hist.most + theme_networks
  
```


number.issues.by.year.networks <- data.networks %>%
  group_by(year) %>%
  count(issue) %>%
  select(year, issue) %>%
  count(year)

#calculating the number or issues by year

article.year.network <- ggplot(number.issues.by.year.networks, aes(x = year, y = n, fill = year)) +
  geom_col(fill = "#9999CC") +
  geom_text(aes(label = n), vjust = -0.5) + 
  scale_x_discrete(year, , limits)
  xlab("Year") + 
  ylab("Number of articles")

article.year.network + theme_networks

```{r}
#tracking the number of articles about neural networks through the issues in science

#things to add: 
  #lines where intervals of about 5 years go
  #tick marks on the x axis

issues.by.year.bar <- ggplot(number.issues) +
  geom_line(aes(x= volume, y = n.items.by.issue)) +
  scale_y_continuous(breaks = c(0, 15, 30, 45, 60, 75, 90, 105, 120, 135, 150)) +
  
  xlab("Issue") + 
  ylab("Number of articles related to neural networks")

issues.by.year.bar + theme_networks

```

```{r}
article.year.count.all <- ggplot(data.networks) +
  geom_bar(aes(year), fill = c("#9999CC")) +
  scale_x_continuous(breaks = c(2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020))+
  xlab("Year") + 
  ylab("Number of articles")

article.year.count.all + theme_networks
```

```{r}

```

```{r}
#randomly selecting articles to close read
  ##note, this will return different articles every time it is run. I've selected the articles that appeared the first time.

#randomly selecting 2 articles from the whole networks database
sample_n(data.networks, 2)
  #selected: Neural scene representation and rendering, 2018. Vol 360, issue 6394. doi: 10.1126/science.aar6170
  #selected: The biochemical basis of microRNA targeting efficacy, 2019. Vol 366. doi: 10.1126/science.aav1741

#randomly selecting 1 article within the top 25% number of citations
  ## extracting the quantiles of the dataset, upper 75% = 171.5
quantile(data.networks$cited.by, na.rm=TRUE)

data.networks.top.75per <- data.networks %>%
  filter(cited.by > 171.5)
s
sample_n(data.networks.top.75per,1)
  #selected: Terrestrial gross carbon dioxide uptake: Global distribution and covariation with climate. doi: 10.1126/science.1184984

#randomly selecting 1 article from 2019 (the year with the most articles published)
data.networks.2019 <- data.networks %>%
  filter(year == "2019")

sample_n(data.networks.2019, 1)
  #selected: Machine learning transforms how microstates are sampled, 2019. Vol 365. Issue 6457. doi: 10.1126/science.aay2568
```

```{r}
# next steps
  ## doing the data visualizations that i'm interested in
  ## pin pointing what visualizations i'm interested in making and why i'm choosing those
  ## randomly selecting 5 different articles to qual-code

```








